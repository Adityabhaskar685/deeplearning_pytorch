{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Convolutional Neural Network with He Initialization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Imports\n","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:17:30.134887Z","iopub.execute_input":"2023-10-19T13:17:30.135313Z","iopub.status.idle":"2023-10-19T13:17:30.534966Z","shell.execute_reply.started":"2023-10-19T13:17:30.135276Z","shell.execute_reply":"2023-10-19T13:17:30.533850Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Setting and Dataset","metadata":{}},{"cell_type":"code","source":"# settings \n# -------------\n#device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# hyperparamters\nrandom_seed = 1\nlearning_rate = 0.05\nnum_epochs = 10\nbatch_size = 128\n\n# architecture\nnum_classes = 10\n\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root = 'data',\n                              train = True,\n                              transform = transforms.ToTensor(),\n                              download = True)\n\ntest_dataset = datasets.MNIST(root = 'data',\n                             train = False,\n                             transform = transforms.ToTensor())\n\n\ntrain_loader = DataLoader(dataset = train_dataset,\n                         batch_size = batch_size,\n                         shuffle = True)\n\ntest_loader = DataLoader(dataset = test_dataset,\n                        batch_size = batch_size,\n                        shuffle = False)\n\n\n\n# checking the dataset\nfor images, labels in train_loader:\n    print(f\"Image batch dimension: {images.shape}\")\n    print(f\"Image label dimension: {labels.shape}\")\n    break","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:23:41.943737Z","iopub.execute_input":"2023-10-19T13:23:41.944055Z","iopub.status.idle":"2023-10-19T13:23:42.964950Z","shell.execute_reply.started":"2023-10-19T13:23:41.944033Z","shell.execute_reply":"2023-10-19T13:23:42.963403Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 253685229.73it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 53316766.65it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 80846958.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 7878630.59it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Image batch dimension: torch.Size([128, 1, 28, 28])\nImage label dimension: torch.Size([128])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:26:31.245482Z","iopub.execute_input":"2023-10-19T13:26:31.245844Z","iopub.status.idle":"2023-10-19T13:26:31.250820Z","shell.execute_reply.started":"2023-10-19T13:26:31.245819Z","shell.execute_reply":"2023-10-19T13:26:31.249897Z"}}},{"cell_type":"code","source":"class ConvNet(torch.nn.Module):\n    def __init__(self, num_classes):\n        super(ConvNet, self).__init__()\n        \n        # calculate same padding\n        # (w - k + 2 *p)/s + 1 = o\n        # => p = (s(o-1) - 1 + k ) /2\n        \n        # 28 x 28 x 1 => 28x28x4\n        self.conv_1 = torch.nn.Conv2d(in_channels = 1,\n                                     out_channels = 4,\n                                     kernel_size = (3,3),\n                                     stride = (1,1),\n                                     padding = 1) # (1(28-1) -28 + 3)\n        \n        # 28x28x4 => 14x14x4\n        self.pool_1 = torch.nn.MaxPool2d(kernel_size = (2,2),\n                                        stride = (2,2),\n                                        padding = 0) # (1(14-1) - 28 + 2)\n        \n        # 14x14x4 => 14x14x8\n        self.conv_2 = torch.nn.Conv2d(in_channels = 4,\n                                     out_channels = 8,\n                                     kernel_size = (3,3),\n                                     stride = (1,1),\n                                     padding = 1) # (1(14-1) - 14 + 3)\n        \n        # 14x14x16 => 7x7x16\n        self.pool_2 = torch.nn.MaxPool2d(kernel_size = (2,2),\n                                        stride = (2,2),\n                                        padding = 0) # (2 (7-1) - 7 + 3)\n        \n        self.linear_1 = torch.nn.Linear(7*7*8, num_classes)\n        \n        # reinitializing weights using the he initialization\n        for m in self.modules():\n            if isinstance(m, torch.nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight.detach())\n                m.bias.detach().zero_()\n                \n            elif isinstance(m, torch.nn.Linear):\n                nn.init.kaiming_normal_(m.weight.detach())\n                m.bias.detach().zero_()\n                \n    def forward(self,x):\n        out = self.conv_1(x)\n        out = F.relu(out)\n        out = self.pool_1(out)\n        \n        out = self.conv_2(out)\n        out = F.relu(out)\n        out = self.pool_2(out)\n        \n        logits = self.linear_1(out.view(-1,7*7*8))\n        probas = F.softmax(logits, dim = 1)\n        return logits, probas\n    \n    \ntorch.manual_seed(random_seed)\nmodel = ConvNet(num_classes = num_classes)\nmodel = model.to(device)\n\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n        \n","metadata":{"execution":{"iopub.status.busy":"2023-10-19T14:06:58.800538Z","iopub.execute_input":"2023-10-19T14:06:58.800854Z","iopub.status.idle":"2023-10-19T14:06:58.812443Z","shell.execute_reply.started":"2023-10-19T14:06:58.800822Z","shell.execute_reply":"2023-10-19T14:06:58.811408Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2023-10-19T14:07:00.791321Z","iopub.execute_input":"2023-10-19T14:07:00.791643Z","iopub.status.idle":"2023-10-19T14:07:00.799248Z","shell.execute_reply.started":"2023-10-19T14:07:00.791620Z","shell.execute_reply":"2023-10-19T14:07:00.797920Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"ConvNet(\n  (conv_1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool_1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (conv_2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool_2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (linear_1): Linear(in_features=392, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"def compute_accuracy(model, data_loader):\n    correct_pred, num_examples = 0,0\n    for features, targets in data_loader:\n        features = features.to(device)\n        targets = targets.to(device)\n        logits, probas = model(features)\n        _, predicted_labels = torch.max(probas, 1)\n        num_examples += targets.size(0)\n        correct_pred += (predicted_labels == targets).sum()\n    return correct_pred.float() / num_examples  * 100\n\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    model = model.train()\n    for batch_idx, (features, targets) in enumerate(train_loader):\n        features = features.to(device)\n        targets = targets.to(device)\n        \n        # forward and backprop\n        logits, probas = model(features)\n        cost = F.cross_entropy(logits,targets)\n        optimizer.zero_grad()\n        \n        cost.backward()\n        \n        # update the model parameters\n        optimizer.step()\n        \n        # logging \n        if not batch_idx % 50:\n            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n                   %(epoch+1, num_epochs, batch_idx, \n                     len(train_loader), cost))\n            \n        \n    model = model.eval()\n    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n          epoch+1, num_epochs, \n          compute_accuracy(model, train_loader)))\n    \n    print(\"Time elapsed: %.2f min\" %((time.time() - start_time) / 60))\n    \nprint(\"Total Training Time: %.2f min\" %((time.time() - start_time) / 60))\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-10-19T14:16:41.817468Z","iopub.execute_input":"2023-10-19T14:16:41.817766Z","iopub.status.idle":"2023-10-19T14:19:13.351985Z","shell.execute_reply.started":"2023-10-19T14:16:41.817743Z","shell.execute_reply":"2023-10-19T14:19:13.351393Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch: 001/010 | Batch 000/469 | Cost: 2.4756\nEpoch: 001/010 | Batch 050/469 | Cost: 1.1251\nEpoch: 001/010 | Batch 100/469 | Cost: 0.7654\nEpoch: 001/010 | Batch 150/469 | Cost: 0.3314\nEpoch: 001/010 | Batch 200/469 | Cost: 0.4457\nEpoch: 001/010 | Batch 250/469 | Cost: 0.3698\nEpoch: 001/010 | Batch 300/469 | Cost: 0.2475\nEpoch: 001/010 | Batch 350/469 | Cost: 0.2565\nEpoch: 001/010 | Batch 400/469 | Cost: 0.1951\nEpoch: 001/010 | Batch 450/469 | Cost: 0.2041\nEpoch: 001/010 training accuracy: 91.98%\nTime elapsed: 0.27 min\nEpoch: 002/010 | Batch 000/469 | Cost: 0.2101\nEpoch: 002/010 | Batch 050/469 | Cost: 0.2950\nEpoch: 002/010 | Batch 100/469 | Cost: 0.1641\nEpoch: 002/010 | Batch 150/469 | Cost: 0.1440\nEpoch: 002/010 | Batch 200/469 | Cost: 0.3373\nEpoch: 002/010 | Batch 250/469 | Cost: 0.1978\nEpoch: 002/010 | Batch 300/469 | Cost: 0.2105\nEpoch: 002/010 | Batch 350/469 | Cost: 0.2003\nEpoch: 002/010 | Batch 400/469 | Cost: 0.1766\nEpoch: 002/010 | Batch 450/469 | Cost: 0.1117\nEpoch: 002/010 training accuracy: 94.99%\nTime elapsed: 0.52 min\nEpoch: 003/010 | Batch 000/469 | Cost: 0.0721\nEpoch: 003/010 | Batch 050/469 | Cost: 0.1077\nEpoch: 003/010 | Batch 100/469 | Cost: 0.1426\nEpoch: 003/010 | Batch 150/469 | Cost: 0.1610\nEpoch: 003/010 | Batch 200/469 | Cost: 0.0336\nEpoch: 003/010 | Batch 250/469 | Cost: 0.2031\nEpoch: 003/010 | Batch 300/469 | Cost: 0.0853\nEpoch: 003/010 | Batch 350/469 | Cost: 0.0892\nEpoch: 003/010 | Batch 400/469 | Cost: 0.1662\nEpoch: 003/010 | Batch 450/469 | Cost: 0.0579\nEpoch: 003/010 training accuracy: 96.01%\nTime elapsed: 0.77 min\nEpoch: 004/010 | Batch 000/469 | Cost: 0.1651\nEpoch: 004/010 | Batch 050/469 | Cost: 0.0995\nEpoch: 004/010 | Batch 100/469 | Cost: 0.1701\nEpoch: 004/010 | Batch 150/469 | Cost: 0.0958\nEpoch: 004/010 | Batch 200/469 | Cost: 0.1079\nEpoch: 004/010 | Batch 250/469 | Cost: 0.1014\nEpoch: 004/010 | Batch 300/469 | Cost: 0.0899\nEpoch: 004/010 | Batch 350/469 | Cost: 0.0584\nEpoch: 004/010 | Batch 400/469 | Cost: 0.1450\nEpoch: 004/010 | Batch 450/469 | Cost: 0.0832\nEpoch: 004/010 training accuracy: 96.71%\nTime elapsed: 1.02 min\nEpoch: 005/010 | Batch 000/469 | Cost: 0.1221\nEpoch: 005/010 | Batch 050/469 | Cost: 0.1125\nEpoch: 005/010 | Batch 100/469 | Cost: 0.0728\nEpoch: 005/010 | Batch 150/469 | Cost: 0.1412\nEpoch: 005/010 | Batch 200/469 | Cost: 0.1228\nEpoch: 005/010 | Batch 250/469 | Cost: 0.0297\nEpoch: 005/010 | Batch 300/469 | Cost: 0.2349\nEpoch: 005/010 | Batch 350/469 | Cost: 0.0970\nEpoch: 005/010 | Batch 400/469 | Cost: 0.1330\nEpoch: 005/010 | Batch 450/469 | Cost: 0.1075\nEpoch: 005/010 training accuracy: 97.10%\nTime elapsed: 1.27 min\nEpoch: 006/010 | Batch 000/469 | Cost: 0.0509\nEpoch: 006/010 | Batch 050/469 | Cost: 0.0489\nEpoch: 006/010 | Batch 100/469 | Cost: 0.0900\nEpoch: 006/010 | Batch 150/469 | Cost: 0.0815\nEpoch: 006/010 | Batch 200/469 | Cost: 0.1059\nEpoch: 006/010 | Batch 250/469 | Cost: 0.1159\nEpoch: 006/010 | Batch 300/469 | Cost: 0.1099\nEpoch: 006/010 | Batch 350/469 | Cost: 0.1068\nEpoch: 006/010 | Batch 400/469 | Cost: 0.0204\nEpoch: 006/010 | Batch 450/469 | Cost: 0.0366\nEpoch: 006/010 training accuracy: 97.34%\nTime elapsed: 1.52 min\nEpoch: 007/010 | Batch 000/469 | Cost: 0.0692\nEpoch: 007/010 | Batch 050/469 | Cost: 0.1003\nEpoch: 007/010 | Batch 100/469 | Cost: 0.0690\nEpoch: 007/010 | Batch 150/469 | Cost: 0.0550\nEpoch: 007/010 | Batch 200/469 | Cost: 0.1088\nEpoch: 007/010 | Batch 250/469 | Cost: 0.1025\nEpoch: 007/010 | Batch 300/469 | Cost: 0.1042\nEpoch: 007/010 | Batch 350/469 | Cost: 0.1803\nEpoch: 007/010 | Batch 400/469 | Cost: 0.1009\nEpoch: 007/010 | Batch 450/469 | Cost: 0.0788\nEpoch: 007/010 training accuracy: 97.68%\nTime elapsed: 1.77 min\nEpoch: 008/010 | Batch 000/469 | Cost: 0.1319\nEpoch: 008/010 | Batch 050/469 | Cost: 0.0337\nEpoch: 008/010 | Batch 100/469 | Cost: 0.0365\nEpoch: 008/010 | Batch 150/469 | Cost: 0.0835\nEpoch: 008/010 | Batch 200/469 | Cost: 0.1056\nEpoch: 008/010 | Batch 250/469 | Cost: 0.1220\nEpoch: 008/010 | Batch 300/469 | Cost: 0.0555\nEpoch: 008/010 | Batch 350/469 | Cost: 0.0513\nEpoch: 008/010 | Batch 400/469 | Cost: 0.1361\nEpoch: 008/010 | Batch 450/469 | Cost: 0.0437\nEpoch: 008/010 training accuracy: 97.73%\nTime elapsed: 2.02 min\nEpoch: 009/010 | Batch 000/469 | Cost: 0.0742\nEpoch: 009/010 | Batch 050/469 | Cost: 0.0970\nEpoch: 009/010 | Batch 100/469 | Cost: 0.1371\nEpoch: 009/010 | Batch 150/469 | Cost: 0.0250\nEpoch: 009/010 | Batch 200/469 | Cost: 0.0577\nEpoch: 009/010 | Batch 250/469 | Cost: 0.0788\nEpoch: 009/010 | Batch 300/469 | Cost: 0.0558\nEpoch: 009/010 | Batch 350/469 | Cost: 0.0848\nEpoch: 009/010 | Batch 400/469 | Cost: 0.0468\nEpoch: 009/010 | Batch 450/469 | Cost: 0.0590\nEpoch: 009/010 training accuracy: 97.79%\nTime elapsed: 2.27 min\nEpoch: 010/010 | Batch 000/469 | Cost: 0.0728\nEpoch: 010/010 | Batch 050/469 | Cost: 0.0216\nEpoch: 010/010 | Batch 100/469 | Cost: 0.0665\nEpoch: 010/010 | Batch 150/469 | Cost: 0.0660\nEpoch: 010/010 | Batch 200/469 | Cost: 0.1245\nEpoch: 010/010 | Batch 250/469 | Cost: 0.0724\nEpoch: 010/010 | Batch 300/469 | Cost: 0.0348\nEpoch: 010/010 | Batch 350/469 | Cost: 0.0519\nEpoch: 010/010 | Batch 400/469 | Cost: 0.0816\nEpoch: 010/010 | Batch 450/469 | Cost: 0.0698\nEpoch: 010/010 training accuracy: 98.07%\nTime elapsed: 2.53 min\nTotal Training Time: 2.53 min\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Test accuracy: %2.f\" %(compute_accuracy(model, test_loader)))","metadata":{"execution":{"iopub.status.busy":"2023-10-19T14:22:38.454296Z","iopub.execute_input":"2023-10-19T14:22:38.454673Z","iopub.status.idle":"2023-10-19T14:22:39.571550Z","shell.execute_reply.started":"2023-10-19T14:22:38.454641Z","shell.execute_reply":"2023-10-19T14:22:39.570608Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Test accuracy: 98\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}